{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1tH1S8nUjcY"
      },
      "source": [
        "# **Habit App - Download, Save and Parse data**\n",
        "\n",
        "* This code is sturctured to work using google colab. \n",
        "* It is designed to connect and pull the data from mongoDB using credentials located in a json file (determined by the variable credentialsFileNameRelativePath)\n",
        "> This code was written by Rani Gera, last editted on November 2022\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y17fJFcHJXYj",
        "outputId": "e0038202-d522-410d-abca-2db2df7f66aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Load Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "id": "Ye0ntmRHVRTk"
      },
      "outputs": [],
      "source": [
        "#@title Import packages\n",
        "# *** In Colab one might need to RUN this part and if there is an error, RESTART THE RUNTIME AND RUN AGAIN) ***\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import dateutil.parser\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Helps make csv like string data to dataframe: (taken from https://stackoverflow.com/questions/22604564/create-pandas-dataframe-from-a-string):\n",
        "import sys\n",
        "if sys.version_info[0] < 3:\n",
        "    from StringIO import StringIO\n",
        "else:\n",
        "    from io import StringIO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "cellView": "form",
        "id": "JwlNCpXfV9QP"
      },
      "outputs": [],
      "source": [
        "#@title Parameter definition { form-width: \"5%\" }\n",
        "manual_exclusions = [] # example:[705, 706]\n",
        "\n",
        "minSubID = [100, 200, 300]\n",
        "maxSubID = [199, 299, 399]\n",
        "\n",
        "doNotExcludedDueToMissingDays = [] # participants not to exclude due to missing days\n",
        "doNotExcludedDueToNotSatisfyingMinDailyEntries = [] # participants not satisfying the minimum amount of daily entries\n",
        "\n",
        "# paths:\n",
        "# main_path = os.path.dirname(os.path.abspath(__file__))\n",
        "main_path = '/content/drive/MyDrive/Experiments/HAS_STUDY/HAS_Analysis'\n",
        "credentialsFileNameRelativePath = 'mongoDB_stuff/mongo_DB_credentials.json'\n",
        "DB_credentials_file = os.path.join(main_path, credentialsFileNameRelativePath)\n",
        "# Raw data file names:\n",
        "rawDataFileName = os.path.join(main_path, 'data/extracted_data/raw_data.json')\n",
        "rawDataAsListFile = 'raw_data.txt'\n",
        "rawDataAsBinaryFile = \"raw_data.pkl\"\n",
        "# Parsed data filenames:\n",
        "filteredDF_File = os.path.join(main_path, 'data/extracted_data/filteredDF.csv')\n",
        "allDataFor_R_File = os.path.join(main_path, 'data/extracted_data/all_data_for_R.csv')\n",
        "coreDataFileName = '/data/extracted_data/core_table.csv'\n",
        "\n",
        "\n",
        "# number of experimental days by group:\n",
        "n_experimental_days = {'short_training': 4, 'long_training': 11, 'long_training_parallel_manipulations': 11}\n",
        "# number of experimental manipulations per subject:\n",
        "n_manipulations = {'short_training': 3, 'long_training': 3, 'long_training_parallel_manipulations': 6}\n",
        "n_only_val_and_deval_manipulations = {'short_training': 2, 'long_training': 2, 'long_training_parallel_manipulations': 4}\n",
        "parallelManiplationDays = {'still_valued_week1': 2, 'still_valued_post_deval_week1': 4}\n",
        "\n",
        "main_manipulation_days = {\n",
        "    'short_training': {'still_valued': 2, 'devaluation': 3, 'still_valued_post_deval': 4},\n",
        "    'long_training': {'still_valued': 9, 'devaluation': 10, 'still_valued_post_deval': 11},\n",
        "    'long_training_parallel_manipulations': {'still_valued': 9, 'devaluation': 10, 'still_valued_post_deval': 11},  \n",
        "}\n",
        "\n",
        "manipulations_renamed = {\n",
        "    'short_training': ['still_valued', 'devaluation', 'still_valued_post_deval'],\n",
        "    'long_training': ['still_valued', 'devaluation', 'still_valued_post_deval'],\n",
        "    'long_training_parallel_manipulations': ['still_valued_week1', 'still_valued_replacing_devaluation', 'still_valued_post_deval_week1', 'still_valued', 'devaluation', 'still_valued_post_deval'],  \n",
        "}\n",
        "\n",
        "all_manipulation_days = {\n",
        "    'short_training': {'still_valued': 2, 'devaluation': 3, 'still_valued_post_deval': 4},\n",
        "    'long_training': {'still_valued': 9, 'devaluation': 10, 'still_valued_post_deval': 11},\n",
        "    'long_training_parallel_manipulations': {'still_valued_week1': 2, 'still_valued_replacing_devaluation': 3, 'still_valued_post_deval_week1': 4, 'still_valued': 9, 'devaluation': 10, 'still_valued_post_deval': 11},  \n",
        "}\n",
        "\n",
        "minimumDailyEntriesRequired = 5\n",
        "\n",
        "# lines to remove in data:\n",
        "signals_to_remove_data_lines = {\n",
        "    'opennedInstallaitonPage': True,\n",
        "    'showInstructions': True,\n",
        "    'instructionsStartedFlag': True,\n",
        "    'isDemo': True\n",
        "}\n",
        "\n",
        "localTimeZone = \"Israel\" # used to adjust the time variables which are recorded in utc time.\n",
        "timeInStringColumns = ['startTime', 'press1Time', 'press2Time', 'outcomeTime', 'endTime', 'userExitOrUnloadTime',\n",
        "                       'realGameBeginsAlertTime', 'realGameBeginsConfirmationTime', 'resetContainerAlertTime', 'resetContainerConfirmationTime', 'manipulationAlertTime', 'manipulationConfirmationTime', 'foundCaveAlertTime', 'foundCaveConfirmationTime', 'endExperimentAlertTime'] # these variables will be converted to the local time zone and saved as datetime variables\n",
        "selected_df_structure = (['subId', 'group', 'day', 'cost', 'reward', 'hideOutcome', 'manipulationToday', 'activateManipulation', 'isUnderManipulation', 'consumptionTest', 'coins_task_hits_count', 'coins_task_misses_count', 'coin_task_finish_status']\n",
        "                         + timeInStringColumns\n",
        "                         + ['dataLoadingTime', 'screenOrientationData', 'touchData', 'isDemo', 'endExperiment', 'uniqueEntryID'])\n",
        "\n",
        "# viewing stuff\n",
        "pd.set_option('max_rows', None)\n",
        "pd.set_option('max_colwidth', None)\n",
        "pd.set_option('display.max_columns', 40)  # set this number to >= your number of cols\n",
        "pd.set_option('display.width', 1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "id": "mHM8b1P-V7hq"
      },
      "outputs": [],
      "source": [
        "#@title Function definition { form-width: \"5%\" }\n",
        "# -----------------------------------------------------------------------\n",
        "def read_mongoextjson_file(\n",
        "        filename):  # adapted from https://stackoverflow.com/a/60850425 to handle the bson files created by MongoDB\n",
        "    with open(filename, \"r\") as f:\n",
        "        # read the entire input; in a real application,\n",
        "        # you would want to read a chunk at a time\n",
        "        bsondata = f.read()\n",
        "\n",
        "        # convert the TenGen JSON to Strict JSON\n",
        "        # here, I just convert the ObjectId and Date structures,\n",
        "        # but it's easy to extend to cover all structures listed at\n",
        "        # http://www.mongodb.org/display/DOCS/Mongo+Extended+JSON\n",
        "        jsondata = re.sub(r'ObjectId\\s*\\(\\s*\\\"(\\S+)\\\"\\s*\\)',\n",
        "                          r'{\"$oid\": \"\\1\"}',\n",
        "                          bsondata)\n",
        "        jsondata = re.sub(r'ISODate\\s*\\(\\s*(\\S+)\\s*\\)',\n",
        "                          r'{\"$date\": \\1}',\n",
        "                          jsondata)\n",
        "        jsondata = re.sub(r'NumberInt\\s*\\(\\s*(\\S+)\\s*\\)',\n",
        "                          r'{\"$numberInt\": \"\\1\"}',\n",
        "                          jsondata)\n",
        "\n",
        "        # now we can parse this as JSON, and use MongoDB's object_hook\n",
        "        # function to get rich Python data structures inside a dictionary\n",
        "        data = json.loads(jsondata, object_hook=json_util.object_hook)\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "def getRawDataAll(DB_credentials_file):\n",
        "  '''\n",
        "  Using a (local) json file which holds the credentials for our Mongo database\n",
        "  to connect and pull the (raw) data into a variable. \n",
        "  '''\n",
        "  with open(DB_credentials_file) as f:\n",
        "    DB_credentials = json.load(f)\n",
        "  client = MongoClient(DB_credentials['host'],\n",
        "                      username=DB_credentials['username'],\n",
        "                      password=DB_credentials['password'],\n",
        "                      authSource=DB_credentials['authSource'],\n",
        "                      authMechanism=DB_credentials['authMechanism'],\n",
        "                      tls=DB_credentials['tls'])\n",
        "  return list(client['nodejs-app'][DB_credentials['collectionName']].find({}))\n",
        "\n",
        "\n",
        "def getRawDataInRange(DB_credentials_file, minSubID, maxSubID):\n",
        "  '''\n",
        "  Using a (local) json file which holds the credentials for our Mongo database\n",
        "  to connect and pull the (raw) data into a variable. \n",
        "  '''\n",
        "  with open(DB_credentials_file) as f:\n",
        "    DB_credentials = json.load(f)\n",
        "  client = MongoClient(DB_credentials['host'],\n",
        "                      username=DB_credentials['username'],\n",
        "                      password=DB_credentials['password'],\n",
        "                      authSource=DB_credentials['authSource'],\n",
        "                      authMechanism=DB_credentials['authMechanism'],\n",
        "                      tls=DB_credentials['tls'])\n",
        "  return list(client['nodejs-app'][DB_credentials['collectionName']].find({'subId':{\"$gte\":minSubID[0],\"$lte\":maxSubID[0]}})) + \\\n",
        "  list(client['nodejs-app'][DB_credentials['collectionName']].find({'subId':{\"$gte\":minSubID[1],\"$lte\":maxSubID[1]}})) + \\\n",
        "  list(client['nodejs-app'][DB_credentials['collectionName']].find({'subId':{\"$gte\":minSubID[2],\"$lte\":maxSubID[2]}}))\n",
        "\n",
        "\n",
        "def getRawDataOneSubject(DB_credentials_file, subject):\n",
        "  '''\n",
        "  Using a (local) json file which holds the credentials for our Mongo database\n",
        "  to connect and pull the (raw) data into a variable. \n",
        "  '''\n",
        "  with open(DB_credentials_file) as f:\n",
        "    DB_credentials = json.load(f)\n",
        "  client = MongoClient(DB_credentials['host'],\n",
        "                      username=DB_credentials['username'],\n",
        "                      password=DB_credentials['password'],\n",
        "                      authSource=DB_credentials['authSource'],\n",
        "                      authMechanism=DB_credentials['authMechanism'],\n",
        "                      tls=DB_credentials['tls'])\n",
        "  return list(client['nodejs-app'][DB_credentials['collectionName']].find({'subId': subject}))\n",
        "\n",
        "\n",
        "def convertToLocalTime(df, timeInStringColumns, localTimeZone):\n",
        "  '''\n",
        "  This funciton converts in the data frame df the column variables indicated in\n",
        "  timeInStringColumns, from strigs of 'ISOdate' to a datetime variable and sets\n",
        "  it to the local time.\n",
        "  The function mutates the input data frame.\n",
        "  '''\n",
        "  for timeVar in timeInStringColumns:\n",
        "    df[timeVar] = data.sub_df[timeVar].map(lambda x: dateutil.parser.parse(x).astimezone(pytz.timezone(localTimeZone)), na_action='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellView": "form",
        "id": "VANcE9JXcv96"
      },
      "outputs": [],
      "source": [
        "#@title Class definition { form-width: \"5%\" }\n",
        "class Paths:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.main_analysis = main_path\n",
        "        self.app_data = os.path.join(main_path, data_path)\n",
        "        self.app_data = os.path.join(self.app_data, app_data_path)\n",
        "        self.app_data_file = os.path.join(self.app_data, app_data_file)\n",
        "\n",
        "\n",
        "class SimulateData:\n",
        "\n",
        "    def __init__(self, df_vars=selected_df_structure, n_subjects=198):\n",
        "        # attributes initiation:\n",
        "        self.core_table = []\n",
        "        self.core_data = []\n",
        "        self.summaryStats = []\n",
        "        self.diff_index_data = []\n",
        "        self.diffSummaryStats = []\n",
        "        self.habitIndex = []\n",
        "        self.habitIndexSummaryStats = []\n",
        "        self.total_entries_per_day = []\n",
        "\n",
        "        # simulate data:\n",
        "        print('Simulating data - suitable for up to 200 participants (edit code for more)')\n",
        "        # initialize some stuff:\n",
        "        manipulationTodayOPTIONS = ['devaluation', 'still_valued']\n",
        "        self.sub_df = pd.DataFrame(columns=df_vars)\n",
        "        n_per_group = int(n_subjects/2)\n",
        "        sub_list = list(range(1701, 1701 + n_per_group)) + list(range(1801, 1801 + n_per_group))\n",
        "        for subId in sub_list:\n",
        "            n_data_point = random.randint(100, 500) if subId % 200 > 100 else random.randint(240, 1200)  # as there are *2.40 days (when including the 5th and 12th day\n",
        "            # set some of the variables:\n",
        "            dic_data = {'subId': [subId] * n_data_point,\n",
        "                    'group': [\"short_training\" if subId % 200 > 100 else \"long_training\"] * n_data_point,\n",
        "                    'day': sorted([random.randint(1,5) for i in range(n_data_point)]) if subId % 200 > 100 else sorted([random.randint(1,12) for i in range(n_data_point)]),\n",
        "                    'cost': [[1]] * n_data_point,\n",
        "                    'reward': random.choices([0,0,15],k=n_data_point),\n",
        "                        }\n",
        "            # set manipulationToday\n",
        "            for i in range(len(dic_data['subId'])):\n",
        "                if dic_data['group'][i] == 'short_training' and dic_data['day'][i] == 3:\n",
        "                    dic_data.setdefault('manipulationToday', []).append(manipulationTodayOPTIONS[subId % 2])\n",
        "                elif dic_data['group'][i] == 'short_training' and dic_data['day'][i] == 4:\n",
        "                    dic_data.setdefault('manipulationToday', []).append(manipulationTodayOPTIONS[abs(subId % 2 -1)])\n",
        "                elif dic_data['group'][i] == 'long_training' and dic_data['day'][i] == 10:\n",
        "                    dic_data.setdefault('manipulationToday', []).append(manipulationTodayOPTIONS[subId % 2])\n",
        "                elif dic_data['group'][i] == 'long_training' and dic_data['day'][i] == 11:\n",
        "                    dic_data.setdefault('manipulationToday', []).append(manipulationTodayOPTIONS[abs(subId % 2 - 1)])\n",
        "                else:\n",
        "                    dic_data.setdefault('manipulationToday', []).append(None)\n",
        "            # set isUnderManipulation\n",
        "            isUnderManipulation = [False] * n_data_point\n",
        "\n",
        "            isUnderManipulation_ind_in_devaluation = round(random.normalvariate(mu=dic_data['manipulationToday'].count('devaluation')/2,sigma=10)-1)\n",
        "            first_isUnderManipulation_ind_in_devaluation = dic_data['manipulationToday'].index('devaluation') + isUnderManipulation_ind_in_devaluation\n",
        "            last_isUnderManipulation_ind_in_devaluation = len(dic_data['manipulationToday']) - 1 - dic_data['manipulationToday'][::-1].index('devaluation')\n",
        "\n",
        "            isUnderManipulation_ind_in_still_devalued = round(random.normalvariate(mu=dic_data['manipulationToday'].count('still_valued')/2,sigma=10)-1)\n",
        "            first_isUnderManipulation_ind_in_still_devalued = dic_data['manipulationToday'].index('still_valued') + isUnderManipulation_ind_in_still_devalued\n",
        "            last_isUnderManipulation_ind_in_still_devalued = len(dic_data['manipulationToday']) - 1 - dic_data['manipulationToday'][::-1].index('still_valued')\n",
        "\n",
        "            isUnderManipulation[first_isUnderManipulation_ind_in_devaluation:last_isUnderManipulation_ind_in_devaluation+1] = [True] * (last_isUnderManipulation_ind_in_devaluation - first_isUnderManipulation_ind_in_devaluation +1)\n",
        "            isUnderManipulation[first_isUnderManipulation_ind_in_still_devalued:last_isUnderManipulation_ind_in_still_devalued+1] = [True] * (last_isUnderManipulation_ind_in_still_devalued - first_isUnderManipulation_ind_in_still_devalued +1)\n",
        "            dic_data['isUnderManipulation'] = isUnderManipulation\n",
        "\n",
        "            # append dict:\n",
        "            self.sub_df = self.sub_df.append(pd.DataFrame.from_dict(dic_data), ignore_index=True)  ###### add self twice\n",
        "\n",
        "    def create_core_table(self, coreTableRelativePath):\n",
        "        self.sub_df.loc[(data.sub_df.group == 'long_training_parallel_manipulations') & (data.sub_df.day == (parallelManiplationDays['still_valued_week1'])) & (data.sub_df.manipulationToday == 'still_valued'),'manipulationToday']  = 'still_valued_week1'\n",
        "        self.sub_df.loc[(data.sub_df.group == 'long_training_parallel_manipulations') & (data.sub_df.day == (parallelManiplationDays['still_valued_post_deval_week1'])) & (data.sub_df.manipulationToday == 'still_valued_post_deval'),'manipulationToday']  = 'still_valued_post_deval_week1'\n",
        "        groupingVars = [\"subId\", \"group\", \"manipulationToday\", \"isUnderManipulation\"]\n",
        "        print('** Grouping by:')\n",
        "        print(groupingVars)\n",
        "        # do the grouping:\n",
        "        groupedData = self.sub_df.groupby(by=groupingVars).size().unstack(fill_value=0).stack()\n",
        "        # change and set names\n",
        "        groupedData.index.names = ['subID', 'group', 'manipulation', 'time']\n",
        "        groupedData = groupedData.rename(index={False: 'pre', True: 'post'})\n",
        "        groupedData.name = 'n_entries'\n",
        "        print('** The data:')\n",
        "        print(groupedData)\n",
        "        groupedData.to_csv(main_path + coreTableRelativePath)\n",
        "        print('** csv file saved to ' + coreTableRelativePath)\n",
        "        self.core_table = pd.read_csv(StringIO(groupedData.to_csv()))\n",
        "        return self.core_table\n",
        "\n",
        "    def core_bar_plots(self):\n",
        "        # initialize vars (tables):\n",
        "        groups = self.sub_df.group.unique()\n",
        "        # making the data ready for ploting:\n",
        "        core_data = self.core_table.set_index(['subID', 'group', 'manipulation', 'time'])\n",
        "        summaryStats = self.core_table.groupby(['group', 'manipulation', 'time']).n_entries.describe()\n",
        "        summaryStats['std_err'] = self.core_table.groupby(['group', 'manipulation', 'time']).n_entries.sem()\n",
        "        print(summaryStats)\n",
        "        diff_index_data = core_data.groupby(level=[0, 1, 2], axis=0).diff().dropna().reset_index('time', drop=True) # post - pre\n",
        "        diffSummaryStats = diff_index_data.groupby(['group', 'manipulation']).n_entries.describe()\n",
        "        diffSummaryStats['std_err'] = diff_index_data.groupby(['group', 'manipulation']).n_entries.sem()\n",
        "        habitIndex = diff_index_data.groupby(level=[0, 1], axis=0).diff().dropna().reset_index('manipulation', drop=True) # [valued post - pre] minus [devalued post - pre]\n",
        "        habitIndexSummaryStats = habitIndex.groupby(['group']).n_entries.describe()\n",
        "        total_entries_per_day = pd.DataFrame(self.sub_df.groupby(by=['subId', 'group', 'day']).size().unstack(fill_value=np.nan).stack()).rename(\n",
        "            columns={0: 'n_entries'}).reset_index().pivot(index='subId', columns=['day'], values='n_entries')\n",
        "\n",
        "        # save tables\n",
        "        self.core_data = core_data\n",
        "        self.summaryStats = summaryStats\n",
        "        self.diff_index_data = diff_index_data\n",
        "        self.diffSummaryStats = diffSummaryStats\n",
        "        self.habitIndex = habitIndex\n",
        "        self.habitIndexSummaryStats = habitIndexSummaryStats\n",
        "        self.total_entries_per_day = total_entries_per_day\n",
        "\n",
        "        # bar plot per group\n",
        "        for group in groups:\n",
        "            plt.figure()\n",
        "            barWidth = 0.3  # width of the bars\n",
        "            bars1 = [summaryStats.loc[group, 'still_valued', 'pre']['mean'], summaryStats.loc[group,'devaluation','pre']['mean']]  # Choose the height of the blue bars\n",
        "            bars2 = [summaryStats.loc[group, 'still_valued', 'post']['mean'], summaryStats.loc[group,'devaluation','post']['mean']]  # Choose the height of the cyan bars\n",
        "            yer1 =  [summaryStats.loc[group, 'still_valued', 'pre']['std_err'], summaryStats.loc[group,'devaluation','pre']['std_err']]  # Choose the height of the error bars (bars1)\n",
        "            yer2 =  [summaryStats.loc[group, 'still_valued', 'post']['std_err'], summaryStats.loc[group,'devaluation','post']['std_err']]  # Choose the height of the error bars (bars2)\n",
        "            # The x position of bars\n",
        "            r1 = np.arange(len(bars1))\n",
        "            r2 = [x + barWidth for x in r1]\n",
        "            # Create blue bars\n",
        "            plt.bar(r1, bars1, width=barWidth, color='blue', edgecolor='black', yerr=yer1, capsize=7, label='pre', alpha=0.5, zorder=0)\n",
        "            # Create cyan bars\n",
        "            plt.bar(r2, bars2, width=barWidth, color='cyan', edgecolor='black', yerr=yer2, capsize=7, label='post', alpha=0.5, zorder=0)\n",
        "            # general layout\n",
        "            plt.xticks(r1 + 0.5 * barWidth, ['valued', 'devalued'])\n",
        "            plt.ylabel('# entries')\n",
        "            plt.title(group)\n",
        "            # add individual data points\n",
        "            for subID in core_data.index.get_level_values('subID').unique():\n",
        "                if core_data.loc[subID].index.get_level_values('group')[0] == group:\n",
        "                    subUniqueColor = np.random.rand(3)*0.9\n",
        "                    plt.plot(np.array([r1[0], r2[0]]), np.array([core_data.loc[subID, group, 'still_valued', 'pre'].n_entries, core_data.loc[subID, group, 'still_valued', 'post'].n_entries]), zorder=0, marker='o', linewidth=1, markersize=2, color=subUniqueColor, alpha=0.3)\n",
        "                    plt.plot(np.array([r1[1], r2[1]]), np.array([core_data.loc[subID, group, 'devaluation', 'pre'].n_entries, core_data.loc[subID, group, 'devaluation', 'post'].n_entries]), zorder=0, marker='o', linewidth=1, markersize=2, color=subUniqueColor, alpha=0.3)\n",
        "\n",
        "            plt.legend(loc='upper center')\n",
        "            plt.show()\n",
        "\n",
        "        # bar plot diff:\n",
        "        plt.figure()\n",
        "        barWidth = 0.3  # width of the bars\n",
        "        try:\n",
        "          bars1 = [diffSummaryStats.loc['short_training', 'still_valued']['mean'], diffSummaryStats.loc['long_training','still_valued']['mean']]  # Choose the height of the blue bars\n",
        "          bars2 = [diffSummaryStats.loc['short_training', 'devaluation']['mean'], diffSummaryStats.loc['long_training','devaluation']['mean']]  # Choose the height of the cyan bars\n",
        "          yer1 =  [diffSummaryStats.loc['short_training', 'still_valued']['std_err'], diffSummaryStats.loc['long_training','still_valued']['std_err']]  # Choose the height of the error bars (bars1)\n",
        "          yer2 =  [diffSummaryStats.loc['short_training', 'devaluation']['std_err'], diffSummaryStats.loc['long_training','devaluation']['std_err']]  # Choose the height of the error bars (bars2)\n",
        "        except:\n",
        "          bars1 = [diffSummaryStats.loc['short_training', 'still_valued']['mean']]  # Choose the height of the blue bars\n",
        "          bars2 = [diffSummaryStats.loc['short_training', 'devaluation']['mean']]  # Choose the height of the cyan bars\n",
        "          yer1 =  [diffSummaryStats.loc['short_training', 'still_valued']['std_err']]  # Choose the height of the error bars (bars1)\n",
        "          yer2 =  [diffSummaryStats.loc['short_training', 'devaluation']['std_err']]  # Choose the height of the error bars (bars2)\n",
        "        # The x position of bars\n",
        "        r1 = np.arange(len(bars1))\n",
        "        r2 = [x + barWidth for x in r1]\n",
        "        # Create blue bars\n",
        "        plt.bar(r1, bars1, width=barWidth, color='blue', edgecolor='black', yerr=yer1, capsize=7, label='valued', alpha=0.5, zorder=0)\n",
        "        # Create cyan bars\n",
        "        plt.bar(r2, bars2, width=barWidth, color='cyan', edgecolor='black', yerr=yer2, capsize=7, label='devalued', alpha=0.5, zorder=0)\n",
        "        # general layout\n",
        "        plt.xticks(r1 + 0.5 * barWidth, ['short_training', 'long_training'])\n",
        "        plt.ylabel('Diff # entries')\n",
        "        plt.title('Sensitivity to outcome devaluation [difference index]')\n",
        "        # add individual data points\n",
        "        for subID in diff_index_data.index.get_level_values('subID').unique():\n",
        "            #subUniqueColor = np.random.rand(3) * 0.9\n",
        "            if diff_index_data.loc[subID].index.get_level_values('group')[0] == 'short_training':\n",
        "                plt.plot(np.array([r1[0], r2[0]]), np.array([diff_index_data.loc[subID, 'short_training', 'still_valued'].n_entries, diff_index_data.loc[subID, 'short_training', 'devaluation'].n_entries]),zorder=0, marker='o', linewidth=1, markersize=2, alpha=0.3)\n",
        "            elif diff_index_data.loc[subID].index.get_level_values('group')[0] == 'long_training':\n",
        "                plt.plot(np.array([r1[1], r2[1]]), np.array([diff_index_data.loc[subID, 'long_training', 'still_valued'].n_entries, diff_index_data.loc[subID, 'long_training', 'devaluation'].n_entries]), zorder=0, marker='o', linewidth=1, markersize=2, alpha=0.3)\n",
        "        plt.legend(loc='upper center')\n",
        "        plt.show()\n",
        "\n",
        "        # create 'learning' curve figure (using inner function):\n",
        "        # -------------------------------------------------------\n",
        "        def createLearningCurve(days_data=total_entries_per_day, individualLines=False):  # a local funciton\n",
        "            shortTraining = days_data.loc[days_data.index % 200 > 100]\n",
        "            longTraining = days_data.loc[days_data.index % 200 < 100]\n",
        "            \n",
        "            # get the means and fill empty days with zero (in case participants entered following the experiment completion)\n",
        "            daysDataMean=days_data.mean(skipna=True)\n",
        "            daysDataSEM=days_data.sem(skipna=True)\n",
        "            for i in range(int(daysDataMean.index.max())):\n",
        "              if i+1 not in daysDataMean.index:\n",
        "                daysDataMean[float(i+1)]=0\n",
        "                daysDataSEM[float(i+1)]=0\n",
        "            daysDataMean.sort_index()\n",
        "            daysDataSEM.sort_index()\n",
        "\n",
        "            shortTrainingMean=shortTraining.mean(skipna=True)\n",
        "            shortTrainingSEM=shortTraining.sem(skipna=True)\n",
        "            for i in range(int(shortTrainingMean.index.max())):\n",
        "              if i+1 not in shortTrainingMean.index:\n",
        "                shortTrainingMean[float(i+1)]=0\n",
        "                shortTrainingSEM[float(i+1)]=0\n",
        "            shortTrainingMean.sort_index()\n",
        "            shortTrainingSEM.sort_index()\n",
        "\n",
        "            longTrainingMean=longTraining.mean(skipna=True)\n",
        "            longTrainingSEM=longTraining.sem(skipna=True)\n",
        "            for i in range(int(longTrainingMean.index.max())):\n",
        "              if i+1 not in longTrainingMean.index:\n",
        "                longTrainingMean[float(i+1)]=0\n",
        "                longTrainingSEM[float(i+1)]=0\n",
        "            longTrainingMean.sort_index()\n",
        "            longTrainingSEM.sort_index()\n",
        "\n",
        "            # plot\n",
        "            plt.figure()\n",
        "            if individualLines:  # create individual lines\n",
        "                plt.plot(shortTraining.T, color='blue', alpha=0.1)\n",
        "                try:\n",
        "                  plt.plot(longTraining.T, color='red', alpha=0.1)\n",
        "                except: \n",
        "                  pass\n",
        "            # plt.plot(total_entries_per_day.T, color='black', alpha=0.1) # all together\n",
        "            # mean lines\n",
        "            plt.errorbar(daysDataMean.index, daysDataMean, color='green', label='All', yerr=daysDataSEM, capsize=7, alpha=0.5, marker='o', linewidth=2)\n",
        "            plt.errorbar(shortTrainingMean.index, shortTrainingMean, color='blue', label='Short-Training', yerr=shortTrainingSEM, capsize=7, alpha=0.5, marker='o', linewidth=2)\n",
        "            try:\n",
        "              plt.errorbar(longTrainingMean.index, longTrainingMean, color='red', label='Long-Training', yerr=longTrainingSEM, capsize=7, alpha=0.5, marker='o', linewidth=2)\n",
        "            except: \n",
        "              pass\n",
        "            plt.legend()\n",
        "            plt.xlabel('Day')\n",
        "            plt.ylabel('# entries')\n",
        "            plt.title('\"learning curve\"')\n",
        "            plt.show()\n",
        "\n",
        "        createLearningCurve()\n",
        "        createLearningCurve(individualLines=True)\n",
        "\n",
        "\n",
        "class Data(SimulateData):\n",
        "    def __init__(self, minSubID = None, maxSubID = None):\n",
        "        # GET DATA:\n",
        "        # OPTION #1 [deprecated by Rani]\n",
        "        # -------------------------------------------------------------\n",
        "        # The commented part below is to handle an array of json which are not in the bson format:\n",
        "        # with open(Paths().app_data_file, encoding='utf-8') as json_file: [OPTION #1 - deprecated]\n",
        "        #     self.raw = json.load(json_file)\n",
        "        # OPTION #2 [still relying on a text file produced by Robo3T]\n",
        "        # -------------------------------------------------------------\n",
        "        # self.raw = read_mongoextjson_file(\n",
        "        #     Paths().app_data_file)  # this is based on downloading the data from \"robo 3t\" after using db.getCollection('sessions').find({}).toArray() in the query and save to a file the from array.\n",
        "        # OPTION #3 - get data directly from the data base (uses a credentials file on the drive):\n",
        "        if not minSubID or not maxSubID:\n",
        "          self.raw = getRawDataAll(DB_credentials_file)        \n",
        "        else:\n",
        "          self.raw = getRawDataInRange(DB_credentials_file, minSubID, maxSubID)\n",
        "        # initialize stuff\n",
        "        self.trials_data = []\n",
        "        self.subject_list = []\n",
        "        self.sub_dict = {}\n",
        "        self.sub_df_list = []\n",
        "        self.sub_df = []\n",
        "\n",
        "    # Get methods from SimulateData\n",
        "    create_core_table = SimulateData.__dict__[\"create_core_table\"]\n",
        "    core_bar_plots = SimulateData.__dict__[\"core_bar_plots\"]\n",
        "\n",
        "    def filter_trials_data(self):\n",
        "        # remove unwanted lines:\n",
        "        for dic in self.raw:\n",
        "            include = True\n",
        "            for key, val in signals_to_remove_data_lines.items():\n",
        "                if key in dic and dic[key] == val:\n",
        "                    include = False\n",
        "                    break\n",
        "            if include:\n",
        "                self.trials_data.append(dic)\n",
        "\n",
        "    def create_sub_dict(self, json_array_data=None):  # create an ordered dictionary of subject IDs as keys:\n",
        "        if json_array_data is None:\n",
        "            json_array_data = self.trials_data\n",
        "        sub_regular_dict = {}\n",
        "        for dic in json_array_data:\n",
        "            sub_regular_dict.setdefault(dic['subId'], []).append(dic)\n",
        "        self.sub_dict = collections.OrderedDict(sorted(sub_regular_dict.items()))\n",
        "\n",
        "    def remove_manual_exclusions(self, sub_dict=None):\n",
        "        if sub_dict is None:\n",
        "            sub_dict = self.sub_dict\n",
        "        for subID in manual_exclusions:\n",
        "            sub_dict.pop(subID, None)\n",
        "\n",
        "    def remove_data_based_exclusions(self, sub_dict=None, remove_subs_without_a_day_that_exceeds_the_last_day=False, requireAllManipulatios=False):\n",
        "        print('** Removing data based exclusions **')\n",
        "        print('-- removing subjects without a day that exceeds the last day is set to: ' + str(\n",
        "            remove_subs_without_a_day_that_exceeds_the_last_day))\n",
        "        if sub_dict is None:\n",
        "            sub_dict = self.sub_dict\n",
        "        \n",
        "        # remove subjects who didn't end the experiment:\n",
        "        subjects = list(sub_dict.keys())\n",
        "        for subID in subjects:\n",
        "            particularSubDF = pd.DataFrame(sub_dict[subID])       \n",
        "            try: # NOTE: if there is an error maybe remove here the KeyError from the 'except' below.. but make sure it's ok.\n",
        "                subGroup = particularSubDF[particularSubDF.group.notna()].group.iloc[-1]\n",
        "                n_expectedDaysOfExperiment = n_experimental_days[subGroup]\n",
        "            except KeyError:\n",
        "                print('subject ' + str(\n",
        "                    subID) + ' appears not to have a group variable on their last entry, removing subject.')\n",
        "                sub_dict.pop(subID, None)\n",
        "                continue  \n",
        "\n",
        "            actualDaysOfExperiment = particularSubDF[(particularSubDF.day.notna()) & (particularSubDF.day <= n_expectedDaysOfExperiment)].day.unique()\n",
        "            n_actualDaysOfExperiment = len(actualDaysOfExperiment)\n",
        "            last_day_listed_for_this_subject = particularSubDF[particularSubDF.day.notna()].day.iloc[-1]\n",
        "\n",
        "            if n_expectedDaysOfExperiment != n_actualDaysOfExperiment: # removing subject that did not enter at least once at each day\n",
        "                if (n_actualDaysOfExperiment == n_expectedDaysOfExperiment-1) & (actualDaysOfExperiment[-1] != n_expectedDaysOfExperiment): #checks if only the last day is missing (with the post-deval manipulation)\n",
        "                  removeData = input('> subject ' + str(subID) + ' is missing his last day *only*. Do you want to REMOVE its data [y/n]? ') == 'y'\n",
        "                else:\n",
        "                  removeData = True\n",
        "                if removeData and (subID not in doNotExcludedDueToMissingDays):  \n",
        "                  print('REMOVING ' + str(subID), '. Reason: did not commit entry on every day.')\n",
        "                  sub_dict.pop(subID, None)\n",
        "                  continue\n",
        "\n",
        "            # Remove subject who do not satisfy the minimum amount of daily entries:\n",
        "            subEntriesWithOutcomeTime = particularSubDF[particularSubDF.outcomeTime.notna()]\n",
        "            subEntriesWithOutcomeTime = subEntriesWithOutcomeTime.drop_duplicates(subset=['subId', 'startTime'], keep='last') # GET RID of double rows with the EXACT same start time (rare but may happen)\n",
        "            daysSatisfyingMinimumDailyEntries = np.sum(subEntriesWithOutcomeTime['day'].value_counts() >= minimumDailyEntriesRequired)\n",
        "            if n_expectedDaysOfExperiment != daysSatisfyingMinimumDailyEntries and (subID not in doNotExcludedDueToNotSatisfyingMinDailyEntries):  # removing subject that did not enter at least once at each day\n",
        "                print('REMOVING ' + str(subID), '. Reason: did not commit the MINIMUM amount of DAILY ENTRIES each day.')\n",
        "                sub_dict.pop(subID, None)\n",
        "                continue\n",
        "\n",
        "            if last_day_listed_for_this_subject == n_expectedDaysOfExperiment: # update if the subject did not enter after the last day (and saw the end game message)\n",
        "                print('* subject ' + str(subID) + ' last day listed do not exceed the expected last day.')\n",
        "                if remove_subs_without_a_day_that_exceeds_the_last_day:\n",
        "                    print('REMOVING ' + str(subID))\n",
        "                    sub_dict.pop(subID, None)\n",
        "\n",
        "            if 'manipulationConfirmationTime' not in particularSubDF:\n",
        "                  print('REMOVING ' + str(subID), '- Reason: Did not see confirm any manipulation (do not have \"manipulationConfirmationTime\" variable in its data frame')\n",
        "                  sub_dict.pop(subID, None)\n",
        "            elif requireAllManipulatios: # removing subjects that did not see sufficient number of manipulations\n",
        "              # n_all_manipulations = len(particularSubDF[(particularSubDF.activateManipulation == True) & (particularSubDF.endTime.notna())])\n",
        "              n_all_manipulations = len(particularSubDF[(particularSubDF.activateManipulation == True) & (particularSubDF.manipulationConfirmationTime.notna())].drop_duplicates(subset='day', keep=\"last\"))\n",
        "              if n_all_manipulations != n_manipulations[subGroup]:\n",
        "                  print('REMOVING ' + str(subID), '- Reason: Did not see all manipulations (the also consider the post devaluation manipulation. * change method arguments to change this.')\n",
        "                  sub_dict.pop(subID, None)\n",
        "            else:\n",
        "              #n_deval_and_still_val_manipulations = len(particularSubDF[(particularSubDF.activateManipulation == True) & (particularSubDF.endTime.notna()) &\n",
        "              #                                                          ((particularSubDF.manipulationToday == 'devaluation') | (particularSubDF.manipulationToday == 'still_valued'))])\n",
        "              n_deval_and_still_val_manipulations = len(particularSubDF[(particularSubDF.activateManipulation == True) & (particularSubDF.manipulationConfirmationTime.notna()) &\n",
        "                                                                        ((particularSubDF.manipulationToday == 'devaluation') | (particularSubDF.manipulationToday == 'still_valued') | (particularSubDF.manipulationToday == 'still_valued_replacing_devaluation'))].drop_duplicates(subset='day', keep=\"last\"))\n",
        "              if n_deval_and_still_val_manipulations != n_only_val_and_deval_manipulations[subGroup]:\n",
        "                  print('REMOVING ' + str(subID), '- Reason: Did not see all devaluation and still-valued manipulations.')\n",
        "                  sub_dict.pop(subID, None)\n",
        "        print('** Removing data based exclusions: COMPLETED **')\n",
        "\n",
        "    def sub_dict_2_df(self, sub_dict=None):\n",
        "        if sub_dict is None:\n",
        "            sub_dict = self.sub_dict\n",
        "        for subID in sub_dict.keys():\n",
        "            if 'df' in locals():\n",
        "                df = df.append(pd.DataFrame.from_dict(sub_dict[subID]), ignore_index=True)\n",
        "            else:\n",
        "                df = pd.DataFrame.from_dict(sub_dict[subID])\n",
        "        self.sub_df = df\n",
        "\n",
        "    def customize_df_structure(self, ordered_col_list=selected_df_structure):\n",
        "        self.sub_df = self.sub_df[ordered_col_list]\n",
        "        convertToLocalTime(self.sub_df, timeInStringColumns, localTimeZone)\n",
        "\n",
        "    def get_subject_list(self, sub_dict=None):\n",
        "        if sub_dict is None:\n",
        "            sub_dict = self.sub_dict\n",
        "        self.subject_list = sorted(list(sub_dict.keys()))\n",
        "\n",
        "    def remove_manual_exclusions_from_sub_list(self):\n",
        "        if self.subject_list:  # check that the list isn't empty\n",
        "            for subID in manual_exclusions:\n",
        "                if subID in self.subject_list:\n",
        "                    self.subject_list.remove(subID)\n",
        "\n",
        "    def runDataBuilderPipeline(self, requireAllManipulatios=False):\n",
        "      print('    Extracting and Bulding the data    ')\n",
        "      print('---------------------------------------')\n",
        "      print('>>> create data.trials_data')\n",
        "      self.filter_trials_data()  # creates data.trials_data\n",
        "      print('>>> create data.sub_dict')\n",
        "      self.create_sub_dict()  # creates data.sub_dict (defaulted to create it from data.trials_data)\n",
        "      print('>>> remove manual exclusions from data.sub_dict')\n",
        "      self.remove_manual_exclusions()  # remove manual exclusions from data.sub_dict\n",
        "      print('>>> remove data based exclusions')\n",
        "      self.remove_data_based_exclusions(remove_subs_without_a_day_that_exceeds_the_last_day=False, requireAllManipulatios = requireAllManipulatios)\n",
        "      print('>>> create a data frame')\n",
        "      self.sub_dict_2_df()  # create pd data frame\n",
        "      print('>>> customize data frame structure')\n",
        "      self.customize_df_structure(ordered_col_list=selected_df_structure)  # arrange it according to definition in the parameters (also convert to local time)\n",
        "      #data.get_subject_list()  # creates data.subject_list\n",
        "      #data.remove_manual_exclusions_from_sub_list()  # remove manual exclusions from data.subject_list\n",
        "      print('>>> COMPLETED')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA0JB6uA6woC"
      },
      "source": [
        "# Read and Parse Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cYn1brUIcplZ"
      },
      "outputs": [],
      "source": [
        "#@title Load and parse data { form-width: \"5%\" }\n",
        "data = Data(minSubID, maxSubID) # instanciates and creates the data.raw\n",
        "data.runDataBuilderPipeline(requireAllManipulatios=True)\n",
        "#data.get_subject_list()  # creates data.subject_list\n",
        "#data.remove_manual_exclusions_from_sub_list()  # remove manual exclusions from data.subject_list\n",
        "\n",
        "data.sub_df = data.sub_df[(data.sub_df.isDemo == False) & (data.sub_df.startTime.notna())] # get rid of non real trial stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "id": "Iv-PAqEe1mFV"
      },
      "outputs": [],
      "source": [
        "#@title Smart remove double lines { form-width: \"5%\" }\n",
        "\n",
        "vars_records_in_reversed_temporal_order = ['endTime', 'coin_task_finish_status', 'coins_task_hits_count', 'foundCaveConfirmationTime', 'foundCaveAlertTime', \\\n",
        "                                  'manipulationConfirmationTime', 'manipulationAlertTime', 'outcomeTime', 'press2Time', 'press1Time', \\\n",
        "                                  'resetContainerConfirmationTime', 'resetContainerAlertTime', 'realGameBeginsConfirmationTime', 'realGameBeginsAlertTime',  'startTime']\n",
        "\n",
        "# iterate subjects\n",
        "for subj in data.sub_df.subId.unique():\n",
        "  sub_data = data.sub_df[data.sub_df.subId==subj] # get subject's data\n",
        "  sub_data_duplicated = sub_data[sub_data.duplicated(['startTime'], keep=False)] # get subject's duplicates\n",
        "  for similarStartTimes in sub_data_duplicated.startTime.unique(): # iterate each duplicated startTime value\n",
        "    similarStartTimeArray = sub_data_duplicated[sub_data_duplicated.startTime == similarStartTimes]  # crate a DF of the duplicated startTime value repetitions\n",
        "    for recorded_var in vars_records_in_reversed_temporal_order: # iterate the relevant variables from the later to the earlier (time-wise) time they are expected to be formed\n",
        "      rows_with_recorde_var = similarStartTimeArray[similarStartTimeArray[recorded_var].notna()] # get the rows that contain a not NA value of it \n",
        "      if rows_with_recorde_var.shape[0]: # check if the variable exists in at least one row\n",
        "        if recorded_var == 'coins_task_hits_count': # if the variable is coins_task_hits_count keep the one with the maximum value\n",
        "          indicesToRemove = rows_with_recorde_var.index[rows_with_recorde_var.index != rows_with_recorde_var.index[np.argmax(rows_with_recorde_var.coins_task_hits_count)]] # get the index of the row with the highest value of coins_task_hits_count and then take the other rows to get rid of later\n",
        "        else:\n",
        "          indicesToRemove = similarStartTimeArray.index[similarStartTimeArray.index != rows_with_recorde_var.index[0]] # get duplicated rows to remove from the data\n",
        "        data.sub_df = data.sub_df.drop(indicesToRemove) # remove all other duplicated rows from the data\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EXnpXAeQF19"
      },
      "source": [
        "### Check if and who cheated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58Pl5x2B8ij9"
      },
      "source": [
        "#### Some data eyeballing QA stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5Dw2HUYd33X"
      },
      "outputs": [],
      "source": [
        "pd.set_option('max_rows', None)\n",
        "XX = data.sub_df[(data.sub_df.press1Time.isna())]\n",
        "XX.subId.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFk6W97_jVTb"
      },
      "source": [
        "CHECK WHO CHEATED:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZHBIpIIOcXVv"
      },
      "outputs": [],
      "source": [
        "#@title with subsequent entries  { form-width: \"5%\" }\n",
        "\n",
        "dat = data.sub_df.copy()\n",
        "dat['DIFF'] = dat.startTime.diff()\n",
        "c=0\n",
        "for i in range(1,dat.shape[0],10):\n",
        "  #print(dat.DIFF.iloc[i:i+10].std())\n",
        "  if dat.DIFF.iloc[i:i+10].std() < timedelta(seconds=0.05):\n",
        "    print(f'{dat.subId.iloc[i]} * {dat.subId.iloc[i+10]} * {dat.subId.index[i]}')\n",
        "    c+=1\n",
        "  # if i == 101:\n",
        "  #   break\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uVsWsB-7kFXA"
      },
      "outputs": [],
      "source": [
        "#@title with a gap of 2 { form-width: \"5%\" }\n",
        "\n",
        "jump = 2\n",
        "dat = data.sub_df.copy()\n",
        "dat['DIFF'] = dat.startTime.diff()\n",
        "c=0\n",
        "for i in range(1,dat.shape[0],10):\n",
        "  #print(dat.DIFF.iloc[i:i+10].std())\n",
        "  if dat.DIFF.iloc[i:i+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i]} - {dat.subId.iloc[i+20]} - {dat.subId.index[i]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+1:i+1+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+1]} - {dat.subId.iloc[i+1+20]} - {dat.subId.index[i+1]}')\n",
        "    c+=1\n",
        "\n",
        "  # if i == 101:\n",
        "  #   break\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AkoMSFhbt8AL"
      },
      "outputs": [],
      "source": [
        "#@title with a gap of 3 { form-width: \"5%\" }\n",
        "\n",
        "jump = 3\n",
        "dat = data.sub_df.copy()\n",
        "dat['DIFF'] = dat.startTime.diff()\n",
        "c=0\n",
        "for i in range(1,dat.shape[0],10):\n",
        "  #print(dat.DIFF.iloc[i:i+10].std())\n",
        "  if dat.DIFF.iloc[i:i+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i]} - {dat.subId.iloc[i+10]} - {dat.subId.index[i]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+1:i+1+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+1]} - {dat.subId.iloc[i+1+20]} - {dat.subId.index[i+1]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+2:i+2+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+2]} - {dat.subId.iloc[i+2+10]} - {dat.subId.index[i+2]}')\n",
        "    c+=1\n",
        "\n",
        "  # if i == 101:\n",
        "  #   break\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZeXA3Jkct7tP"
      },
      "outputs": [],
      "source": [
        "#@title with a gap of 4 { form-width: \"5%\" }\n",
        "\n",
        "jump = 4\n",
        "dat = data.sub_df.copy()\n",
        "dat['DIFF'] = dat.startTime.diff()\n",
        "c=0\n",
        "for i in range(1,dat.shape[0],10):\n",
        "  #print(dat.DIFF.iloc[i:i+10].std())\n",
        "  if dat.DIFF.iloc[i:i+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i]} - {dat.subId.iloc[i+10]} - {dat.subId.index[i]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+1:i+1+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+1]} - {dat.subId.iloc[i+1+20]} - {dat.subId.index[i+1]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+2:i+2+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+2]} - {dat.subId.iloc[i+2+10]} - {dat.subId.index[i+2]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+3:i+3+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+3]} - {dat.subId.iloc[i+3+10]} - {dat.subId.index[i+3]}')\n",
        "    c+=1\n",
        "\n",
        "  # if i == 101:\n",
        "  #   break\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9MS25gZUvEFU"
      },
      "outputs": [],
      "source": [
        "#@title with a gap of 5  { form-width: \"5%\" }\n",
        "\n",
        "jump = 5\n",
        "dat = data.sub_df.copy()\n",
        "dat['DIFF'] = dat.startTime.diff()\n",
        "c=0\n",
        "for i in range(1,dat.shape[0],10):\n",
        "  #print(dat.DIFF.iloc[i:i+10].std())\n",
        "  if dat.DIFF.iloc[i:i+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i]} - {dat.subId.iloc[i+10]} - {dat.subId.index[i]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+1:i+1+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+1]} - {dat.subId.iloc[i+1+20]} - {dat.subId.index[i+1]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+2:i+2+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+2]} - {dat.subId.iloc[i+2+10]} - {dat.subId.index[i+2]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+3:i+3+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+3]} - {dat.subId.iloc[i+3+10]} - {dat.subId.index[i+3]}')\n",
        "    c+=1\n",
        "  if dat.DIFF.iloc[i+4:i+4+10*jump:jump].std() < timedelta(seconds=0.1):\n",
        "    print(f'{dat.subId.iloc[i+4]} - {dat.subId.iloc[i+4+10]} - {dat.subId.index[i+4]}')\n",
        "    c+=1\n",
        "\n",
        "  # if i == 101:\n",
        "  #   break\n",
        "c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzJlIImWPSUS"
      },
      "source": [
        "## **REMOVING SUBJECTS 354 and 208"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We357gF4PK0r"
      },
      "outputs": [],
      "source": [
        "data.sub_df = data.sub_df[data.sub_df.subId != 354] # for cheating\n",
        "data.sub_df = data.sub_df[data.sub_df.subId != 208] # Recieved one of the manip[ulations twice (probabely an unclear bug)\n",
        "data.sub_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz5Z16DBlSXR"
      },
      "outputs": [],
      "source": [
        "data.create_core_table(coreTableRelativePath=coreDataFileName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "id": "oyb67JVAWUb7"
      },
      "outputs": [],
      "source": [
        "#@title Save the raw data and filtered DF { form-width: \"5%\" }\n",
        "from bson import ObjectId\n",
        "import json\n",
        "class JSONEncoder(json.JSONEncoder):\n",
        "    def default(self, o):\n",
        "        if isinstance(o, ObjectId):\n",
        "            return str(o)\n",
        "        if isinstance(o, datetime):\n",
        "            return str(o)\n",
        "        return json.JSONEncoder.default(self, o)\n",
        "with open(rawDataFileName, 'w') as fout:\n",
        "    json.dump(data.raw , fout, cls=JSONEncoder)\n",
        "\n",
        "# # load this data:\n",
        "# with open(main_path + '/data/extracted_data/XXX.json', \"r\") as read_file:\n",
        "#     rawData = json.load(read_file)\n",
        "\n",
        "# # Write directly from the DB data cursor:\n",
        "# # create a cursor for everything in range\n",
        "# cursor = client['nodejs-app'][DB_credentials['collectionName']].find({\"$or\": [{'subId':{\"$gte\":minSubID[0],\"$lte\":maxSubID[0]}}, {'subId':{\"$gte\":minSubID[1],\"$lte\":maxSubID[1]}}, {'subId':{\"$gte\":minSubID[2],\"$lte\":maxSubID[2]}}]})\n",
        "# with open(main_path + '/data/extracted_data/XXX.json', 'w') as file:\n",
        "#     file.write('[')\n",
        "#     for document in cursor:\n",
        "#         file.write(json_util.dumps(document))\n",
        "#         file.write(',')\n",
        "#     file.write(']')\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "# Save the filtered data frame:\n",
        "data.sub_df.to_csv(filteredDF_File)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eLQpU2nklz-c"
      },
      "outputs": [],
      "source": [
        "#@title create core_data + Summary Stats\n",
        "# ------- replacing the core_bar_plots method ---------\n",
        "\n",
        "# initialize vars (tables):\n",
        "groups = data.sub_df.group.unique()\n",
        "# making the data ready for ploting:\n",
        "core_data = data.core_table[data.core_table.time == 'post']\n",
        "core_data = core_data.drop(columns='time')\n",
        "core_data = core_data.set_index(['subID', 'group', 'manipulation'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sset6YfbBvAD"
      },
      "outputs": [],
      "source": [
        "#@title Create a DF with columns of averaged still-valued manipulations (+Summary Stats) {form-width: \"1%\"}\n",
        "# (one for the pre and post and one that includes all the relvant ones in the parallel group - i.e. 5 days)\n",
        "# extract and organize\n",
        "core_data_with_averaged_still_valued=core_data.unstack('manipulation')\n",
        "core_data_with_averaged_still_valued.columns = core_data_with_averaged_still_valued.columns.droplevel()\n",
        "# add new vars\n",
        "core_data_with_averaged_still_valued['mean_still_valued'] = core_data_with_averaged_still_valued.loc[:,['still_valued', 'still_valued_post_deval']].mean(axis=1)\n",
        "core_data_with_averaged_still_valued['mean_still_valued_all'] = core_data_with_averaged_still_valued.loc[:,['still_valued',\t'still_valued_post_deval',\t'still_valued_post_deval_week1',\t'still_valued_replacing_devaluation',\t'still_valued_week1']].mean(axis=1, skipna=True)\n",
        "# construct and arange to bein the same structure as core_data:\n",
        "core_data_with_averaged_still_valued.reset_index().set_index(['subID', 'group']).stack('manipulation')\n",
        "core_data_with_averaged_still_valued = core_data_with_averaged_still_valued.drop(columns=['still_valued',\t'still_valued_post_deval',\t'still_valued_post_deval_week1',\t'still_valued_replacing_devaluation',\t'still_valued_week1'])\n",
        "core_data_with_averaged_still_valued = core_data_with_averaged_still_valued.reset_index().set_index(['subID', 'group']).stack('manipulation').to_frame()\n",
        "core_data_with_averaged_still_valued.columns = ['n_entries']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ajblKcnUKfE6"
      },
      "outputs": [],
      "source": [
        "#@title Create a DF with columns of DIFFERENCE measures {form-width: \"1%\"}\n",
        "\n",
        "core_data_with_diffs = core_data.unstack('manipulation')\n",
        "core_data_with_diffs.columns = core_data_with_diffs.columns.droplevel()\n",
        "\n",
        "# add vars:\n",
        "# add mean valued data:\n",
        "core_data_with_diffs['mean_still_valued'] = core_data_with_diffs.loc[:,['still_valued', 'still_valued_post_deval']].mean(axis=1)\n",
        "core_data_with_diffs['mean_still_valued_all'] = core_data_with_diffs.loc[:,['still_valued',\t'still_valued_post_deval',\t'still_valued_post_deval_week1',\t'still_valued_replacing_devaluation',\t'still_valued_week1']].mean(axis=1, skipna=True)\n",
        "# add square root data (to give less weight for high values)\n",
        "core_data_with_diffs['devaluation_SQRT'] = np.sqrt(core_data_with_diffs['devaluation'])\n",
        "core_data_with_diffs['still_valued_SQRT'] = np.sqrt(core_data_with_diffs['still_valued'])\n",
        "core_data_with_diffs['still_valued_post_deval_SQRT'] = np.sqrt(core_data_with_diffs['still_valued_post_deval'])\n",
        "core_data_with_diffs['mean_still_valued_SQRT'] = np.sqrt(core_data_with_diffs['mean_still_valued'])\n",
        "core_data_with_diffs['mean_still_valued_all_SQRT'] = np.sqrt(core_data_with_diffs['mean_still_valued_all'])\n",
        "\n",
        "# add diff vars:\n",
        "core_data_with_diffs['postVal_minus_deval'] = core_data_with_diffs.loc[:,['devaluation', 'still_valued_post_deval']].diff(axis=1).loc[:,'still_valued_post_deval']\n",
        "core_data_with_diffs['stillVal_minus_deval'] = core_data_with_diffs.loc[:,['devaluation', 'still_valued']].diff(axis=1).loc[:,'still_valued']\n",
        "core_data_with_diffs['stillVal_minus_postVal'] = core_data_with_diffs.loc[:,['still_valued_post_deval', 'still_valued']].diff(axis=1).loc[:,'still_valued']\n",
        "core_data_with_diffs['preVal_relativeDiff_deval'] = core_data_with_diffs.apply(lambda x: (x['still_valued'] - x['devaluation'])/(x['devaluation'] + x['still_valued']), axis=1)\n",
        "core_data_with_diffs['postVal_relativeDiff_deval'] = core_data_with_diffs.apply(lambda x: (x['still_valued_post_deval'] - x['devaluation'])/(x['devaluation'] + x['still_valued_post_deval']), axis=1)\n",
        "core_data_with_diffs['preVal_relativeDiff_deval_SQRT'] = core_data_with_diffs.apply(lambda x: (x['still_valued_SQRT'] - x['devaluation_SQRT'])/(x['devaluation_SQRT'] + x['still_valued_SQRT']), axis=1)\n",
        "core_data_with_diffs['postVal_relativeDiff_deval_SQRT'] = core_data_with_diffs.apply(lambda x: (x['still_valued_post_deval_SQRT'] - x['devaluation_SQRT'])/(x['devaluation_SQRT'] + x['still_valued_post_deval_SQRT']), axis=1)\n",
        "\n",
        "core_data_with_diffs['meanVal_minus_deval'] = core_data_with_diffs.loc[:,['devaluation', 'mean_still_valued']].diff(axis=1).loc[:,'mean_still_valued']\n",
        "core_data_with_diffs['meanAllVal_minus_deval'] = core_data_with_diffs.loc[:,['devaluation', 'mean_still_valued_all']].diff(axis=1).loc[:,'mean_still_valued_all']\n",
        "core_data_with_diffs['meanVal_minus_deval_SQRT'] = core_data_with_diffs.loc[:,['devaluation_SQRT', 'mean_still_valued_SQRT']].diff(axis=1).loc[:,'mean_still_valued_SQRT']\n",
        "core_data_with_diffs['meanAllVal_minus_deval_SQRT'] = core_data_with_diffs.loc[:,['devaluation_SQRT', 'mean_still_valued_all_SQRT']].diff(axis=1).loc[:,'mean_still_valued_all_SQRT']\n",
        "core_data_with_diffs['meanVal_relativeDiff_deval'] = core_data_with_diffs.apply(lambda x: (x['mean_still_valued'] - x['devaluation'])/(x['devaluation'] + x['mean_still_valued']), axis=1)\n",
        "core_data_with_diffs['meanAllVal_relativeDiff_deval'] = core_data_with_diffs.apply(lambda x: (x['mean_still_valued_all'] - x['devaluation'])/(x['devaluation'] + x['mean_still_valued_all']), axis=1)\n",
        "core_data_with_diffs['meanVal_relativeDiff_deval_SQRT'] = core_data_with_diffs.apply(lambda x: (x['mean_still_valued_SQRT'] - x['devaluation_SQRT'])/(x['devaluation_SQRT'] + x['mean_still_valued_SQRT']), axis=1)\n",
        "core_data_with_diffs['meanAllVal_relativeDiff_deval_SQRT'] = core_data_with_diffs.apply(lambda x: (x['mean_still_valued_all_SQRT'] - x['devaluation_SQRT'])/(x['devaluation_SQRT'] + x['mean_still_valued_all_SQRT']), axis=1)\n",
        "\n",
        "core_data_with_diffs['prcChange_preToDeval'] = core_data_with_diffs.apply(lambda x: ((x['devaluation'] - x['still_valued'])/x['still_valued'])*100, axis=1)\n",
        "core_data_with_diffs['prcChange_preToPost'] = core_data_with_diffs.apply(lambda x: ((x['still_valued_post_deval'] - x['still_valued'])/x['still_valued'])*100, axis=1)\n",
        "\n",
        "core_data_with_diffs['preVal_relativeDiff_postVal'] = core_data_with_diffs.apply(lambda x: (x['still_valued'] - x['still_valued_post_deval'])/(x['still_valued_post_deval'] + x['still_valued']), axis=1)\n",
        "core_data_with_diffs['preVal_relativeDiff_postVal_SQRT'] = core_data_with_diffs.apply(lambda x: (x['still_valued_SQRT'] - x['still_valued_post_deval_SQRT'])/(x['still_valued_post_deval_SQRT'] + x['still_valued_SQRT']), axis=1)\n",
        "\n",
        "if input(\"Should RelativeDiff values for participants with 0 entries after all manipulatuions turned to 0 (rather than nan)? [y/N] \") == 'y':\n",
        "  print('Turn these values to zeros')\n",
        "  columnsWithRelativeDiff = core_data_with_diffs.columns[['relativeDiff' in x for x in core_data_with_diffs.columns]]\n",
        "  for col in columnsWithRelativeDiff:\n",
        "    print(core_data_with_diffs.loc[core_data_with_diffs[col].isna(),col])\n",
        "    core_data_with_diffs.loc[core_data_with_diffs[col].isna(),col] = 0\n",
        "\n",
        "#Assemble entries variability measures\n",
        "# get number of entries in each day\n",
        "total_entries_per_day = pd.DataFrame(data.sub_df.groupby(by=['subId', 'group', 'day']).size().unstack(fill_value=np.nan).stack()).rename(\n",
        "    columns={0: 'n_entries'}).reset_index().pivot(index='subId', columns=['day'], values='n_entries')\n",
        "\n",
        "# remove data for days that shoudn't be in the calculation\n",
        "total_entries_per_day = total_entries_per_day.loc[:,:n_experimental_days['long_training']]\n",
        "total_entries_per_day.loc[minSubID[0]:maxSubID[0],main_manipulation_days['short_training']['devaluation']] = None\n",
        "total_entries_per_day.loc[minSubID[0]:maxSubID[0],(n_experimental_days['short_training']+1):] = None\n",
        "total_entries_per_day.loc[minSubID[1]:maxSubID[2],main_manipulation_days['long_training']['devaluation']] = None\n",
        "\n",
        "# add the relevant data\n",
        "core_data_with_diffs = core_data_with_diffs.assign(STD_entriesAcrossNoDevalDays=total_entries_per_day.std(axis=1,skipna=True).values)\n",
        "core_data_with_diffs = core_data_with_diffs.assign(SEM_entriesAcrossNoDevalDays=total_entries_per_day.sem(axis=1,skipna=True).values)\n",
        "\n",
        "core_data_with_diffs = core_data_with_diffs.assign(NormedSTD_entriesAcrossNoDevalDays=total_entries_per_day.std(axis=1,skipna=True).values / total_entries_per_day.mean(axis=1,skipna=True).values)\n",
        "core_data_with_diffs = core_data_with_diffs.assign(NormedVar_entriesAcrossNoDevalDays=total_entries_per_day.var(axis=1,skipna=True).values / total_entries_per_day.mean(axis=1,skipna=True).values)\n",
        "\n",
        "# Add first day entries and entries throught the experiment\n",
        "core_data_with_diffs = core_data_with_diffs.assign(firstDayEntries=total_entries_per_day[1].values)\n",
        "core_data_with_diffs = core_data_with_diffs.assign(allEntries=total_entries_per_day.sum(axis=1,skipna=True).values)\n",
        "\n",
        "# -------------------------- Adding similar stuff but after excluding all manipulation days: -----------------------------\n",
        "\n",
        "# get number of entries in each day\n",
        "total_entries_per_day2 = pd.DataFrame(data.sub_df.groupby(by=['subId', 'group', 'day']).size().unstack(fill_value=np.nan).stack()).rename(\n",
        "    columns={0: 'n_entries'}).reset_index().pivot(index='subId', columns=['day'], values='n_entries')\n",
        "\n",
        "# remove data for days that shoudn't be in the calculation\n",
        "total_entries_per_day2 = total_entries_per_day2.loc[:,:11]\n",
        "total_entries_per_day2.loc[100:199,2:] = None\n",
        "total_entries_per_day2.loc[200:400,9:] = None\n",
        "total_entries_per_day2.loc[300:400,2:4] = None\n",
        "\n",
        "# add the relevant data\n",
        "core_data_with_diffs = core_data_with_diffs.assign(STD_entriesAcrossNoManipulationDays=total_entries_per_day2.std(axis=1,skipna=True).values)\n",
        "core_data_with_diffs = core_data_with_diffs.assign(SEM_entriesAcrossNoManipulationDays=total_entries_per_day2.sem(axis=1,skipna=True).values)\n",
        "\n",
        "core_data_with_diffs = core_data_with_diffs.assign(NormedSTD_entriesAcrossNoManipulationDays=total_entries_per_day2.std(axis=1,skipna=True).values / total_entries_per_day2.mean(axis=1,skipna=True).values)\n",
        "core_data_with_diffs = core_data_with_diffs.assign(NormedVar_entriesAcrossNoManipulationDays=total_entries_per_day2.var(axis=1,skipna=True).values / total_entries_per_day2.mean(axis=1,skipna=True).values)\n",
        "\n",
        "# Add first day entries and entries throught the experiment\n",
        "core_data_with_diffs = core_data_with_diffs.assign(allEntriesAcrossNoManipulationDay=total_entries_per_day2.sum(axis=1,skipna=True).values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_dGzUZdojh7"
      },
      "source": [
        "## Assemble data for ITI (self-fromed sessions) and RTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "cellView": "form",
        "id": "x9plHCwSCPBC"
      },
      "outputs": [],
      "source": [
        "#@title Assemble the ITI and RTs data { form-width: \"5%\" }\n",
        "\n",
        "# parameters:\n",
        "timeOfEntranceAnim = 800 # time in ms it should takes the sequence pressing screen to appear:\n",
        "\n",
        "seperatingMicroSessionsTime = 300 # namely if there is 300 seconds apart between subsequent startTime's it will be considered a new session.\n",
        "# helpers:\n",
        "getSecsFunc = np.vectorize(lambda x: x.total_seconds())\n",
        "# ------------------------------------------------------------------------------\n",
        "# Create a new sub_df and add new measures:\n",
        "sub_df_extended = data.sub_df.copy().sort_values(['subId','startTime']) # ******** TO MAKE SURE STARTTIMES ARE IN THE CORRECT ORDER\n",
        "sub_df_extended = sub_df_extended[sub_df_extended.endExperiment!=True] # remove irrelevant days\n",
        "sub_df_extended['deltaStartTime']=np.insert(np.diff(sub_df_extended.startTime),0,None) # add difference between startTime\n",
        "sub_df_extended['deltaSecStartTime']=np.insert(getSecsFunc(np.diff(sub_df_extended.startTime)),0,None) # add difference between startTime\n",
        "sub_df_extended['sessionsBeginning'] = False\n",
        "\n",
        "for sub in sub_df_extended.subId.unique(): # set stuff for the beginning of each subject\n",
        "  sub_df_extended.loc[(sub_df_extended.subId==sub).idxmax(), 'deltaStartTime'] = None\n",
        "  sub_df_extended.loc[(sub_df_extended.subId==sub).idxmax(), 'deltaSecStartTime'] = None\n",
        "  sub_df_extended.loc[(sub_df_extended.subId==sub).idxmax(), 'sessionsBeginning'] = True\n",
        "\n",
        "sub_df_extended.loc[sub_df_extended.deltaSecStartTime > seperatingMicroSessionsTime , 'sessionsBeginning'] = True # Add session beginning (except for the ones on day beginning)\n",
        "\n",
        "sub_df_extended['secsToPress1'] = sub_df_extended.apply(lambda x: (x.press1Time - (x.startTime + timedelta(milliseconds = x.dataLoadingTime) + timedelta(milliseconds = timeOfEntranceAnim))).total_seconds(), axis=1)\n",
        "sub_df_extended['secsToPress2'] = sub_df_extended.apply(lambda x: (x.press2Time - x.press1Time).total_seconds(), axis=1)\n",
        "sub_df_extended['secsToExitAfterOutcomeTime'] = sub_df_extended.apply(lambda x: (x.userExitOrUnloadTime - x.outcomeTime).total_seconds(), axis=1)\n",
        "\n",
        "sub_df_extended['secsToPress2fromEntry'] = sub_df_extended.apply(lambda x: (x.press2Time - (x.startTime + timedelta(milliseconds = x.dataLoadingTime) + timedelta(milliseconds = timeOfEntranceAnim))).total_seconds(), axis=1)\n",
        "sub_df_extended['secsToExitAfterOutcomeTimefromEntry'] = sub_df_extended.apply(lambda x: (x.userExitOrUnloadTime - (x.startTime + timedelta(milliseconds = x.dataLoadingTime) + timedelta(milliseconds = timeOfEntranceAnim))).total_seconds(), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pQVVBxcCsJNO"
      },
      "outputs": [],
      "source": [
        "#@title Assemble a summary table of ITI and RTs data + per day tables { form-width: \"5%\" }\n",
        "\n",
        "for i, sub in enumerate(sub_df_extended.subId.unique()):\n",
        "  # Assemble data\n",
        "  # --------------\n",
        "  # get relevant days and calculate time differences between entries:\n",
        "  sub_data = sub_df_extended[sub_df_extended.subId==sub]\n",
        "  # Remove first entry from each to ignore the large time difference over night\n",
        "  x = sub_data.copy()\n",
        "  for day in x.day.unique():\n",
        "    x = x.drop(index = x[x.day==day].index[0])\n",
        "\n",
        "  # -------------- construct DFs of daily data of session --------------------\n",
        "  sub_n_daily_sessions=(x[x.deltaSecStartTime>300].groupby(['subId','group','day']).count().deltaSecStartTime + 1).reset_index('day').astype({\"day\": int}).pivot(columns='day')\n",
        "  sub_n_daily_sessions.columns = sub_n_daily_sessions.columns.droplevel(0)\n",
        "\n",
        "  sub_n_daily_entries=(sub_data.groupby(['subId','group','day']).count().startTime).reset_index('day').astype({\"day\": int}).pivot(columns='day')\n",
        "  sub_n_daily_entries.columns = sub_n_daily_entries.columns.droplevel(0)\n",
        "\n",
        "  # -------------- Calculate and Add new important measures --------------------\n",
        "  x_devalDropped = x[x.manipulationToday != 'devaluation']\n",
        "  x_allManipulationsDropped = x[x.manipulationToday.isna()]\n",
        "\n",
        "  # Define the number of session as a measure of entries that came after more than 5 minutes from the last entry:\n",
        "  n_sessions = len(x_devalDropped.deltaSecStartTime[x_devalDropped.deltaSecStartTime>300]) + (n_experimental_days[x_devalDropped.iloc[0].group]-1) # for the beginning of each day (except devaluation day);\n",
        "  if x.group.iloc[0] == 'long_training_parallel_manipulations':\n",
        "    n_sessions_no_manipulations = len(x_allManipulationsDropped.deltaSecStartTime[x_allManipulationsDropped.deltaSecStartTime>300]) + (n_experimental_days[x_allManipulationsDropped.iloc[0].group]-6) # for the beginning of each day (except manipulation days);\n",
        "  else:\n",
        "    n_sessions_no_manipulations = len(x_allManipulationsDropped.deltaSecStartTime[x_allManipulationsDropped.deltaSecStartTime>300]) + (n_experimental_days[x_allManipulationsDropped.iloc[0].group]-3) # for the beginning of each day (except manipulation days);\n",
        "\n",
        "  # Define the average sessions per day\n",
        "  averageSessionsPerDay = n_sessions/(n_experimental_days[x_devalDropped.iloc[0].group]-1)\n",
        "  if x.group.iloc[0] == 'long_training_parallel_manipulations':\n",
        "    averageSessionsPerDay_no_manipulations = n_sessions_no_manipulations/(n_experimental_days[x_allManipulationsDropped.iloc[0].group]-6)\n",
        "  else:\n",
        "    averageSessionsPerDay_no_manipulations = n_sessions_no_manipulations/(n_experimental_days[x_allManipulationsDropped.iloc[0].group]-3)\n",
        "\n",
        "  # Define the average entries per session\n",
        "  averageEntriesInSession = (len(x_devalDropped.deltaSecStartTime) + (n_experimental_days[x_devalDropped.iloc[0].group]-1)) / n_sessions\n",
        "  if x.group.iloc[0] == 'long_training_parallel_manipulations':\n",
        "    averageEntriesInSession_no_manipulations = (len(x_allManipulationsDropped.deltaSecStartTime) + (n_experimental_days[x_allManipulationsDropped.iloc[0].group]-6)) / n_sessions_no_manipulations\n",
        "  else:\n",
        "    averageEntriesInSession_no_manipulations = (len(x_allManipulationsDropped.deltaSecStartTime) + (n_experimental_days[x_allManipulationsDropped.iloc[0].group]-3)) / n_sessions_no_manipulations\n",
        "\n",
        "  # Create a table with the different measures\n",
        "  summaryTable = x_devalDropped.deltaSecStartTime.describe()\n",
        "  summaryTable['MAD'] = stats.median_abs_deviation(x_devalDropped.deltaSecStartTime, nan_policy='omit')\n",
        "  summaryTable['Skew'] = x_devalDropped.deltaSecStartTime.skew()\n",
        "  summaryTable['NP_Skew'] = (summaryTable['mean'] - summaryTable['50%']) / summaryTable['std']\n",
        "  summaryTable['Norm_spread'] = summaryTable['std'] / summaryTable['mean']\n",
        "  summaryTable['n_sessions'] = n_sessions\n",
        "  summaryTable['n_sessionsPerDay'] = averageSessionsPerDay\n",
        "  summaryTable['avgSessionEntries'] = averageEntriesInSession\n",
        "  summaryTable['n_sessions_no_manipulations'] = n_sessions_no_manipulations\n",
        "  summaryTable['n_sessionsPerDay_no_manipulations'] = averageSessionsPerDay_no_manipulations\n",
        "  summaryTable['avgSessionEntries_no_manipulations'] = averageEntriesInSession_no_manipulations\n",
        "\n",
        "  # Add measures of number of session and number of entries per session for after each manipulation\n",
        "  for manipulation in sub_data.manipulationToday.unique()[sub_data.manipulationToday.unique()!=None]:\n",
        "    # sessions after manipulations\n",
        "    postManpulationTrialsDeltaSec = x.deltaSecStartTime[(x.manipulationToday == manipulation) & (x.isUnderManipulation)]\n",
        "    summaryTable['n_sessions_' + manipulation] = len(postManpulationTrialsDeltaSec[postManpulationTrialsDeltaSec>300]) + ((postManpulationTrialsDeltaSec.iloc[0]<300) if len(postManpulationTrialsDeltaSec) else 0) # The last term is to consider as a session entries right after the manipulation\n",
        "    summaryTable['avgSessionEntries_' + manipulation] = len(postManpulationTrialsDeltaSec) / summaryTable['n_sessions_' + manipulation]\n",
        "\n",
        "  # assemble within-trial time variables:\n",
        "  # --------------------------------------------\n",
        "  summaryTable['MEANsecsToPress1'] = sub_data.secsToPress1.mean()\n",
        "  summaryTable['MEANsecsToPress2'] = sub_data.secsToPress2.mean()\n",
        "  summaryTable['MEANsecsToExitAfterOutcomeTime'] = sub_data[(sub_data.activateManipulation != True) & (sub_data.secsToExitAfterOutcomeTime<=60)].secsToExitAfterOutcomeTime.mean() # the latter is to exclude unreasonably higher values\n",
        "  summaryTable['MEANsecsToPress2fromEntry'] = sub_data.secsToPress2fromEntry.mean()\n",
        "  summaryTable['MEANsecsToExitAfterOutcomeTimefromEntry'] = sub_data[(sub_data.activateManipulation != True) & (sub_data.secsToExitAfterOutcomeTime<=60)].secsToExitAfterOutcomeTimefromEntry.mean() # the latter is to exclude unreasonably higher values\n",
        "  \n",
        "  for manipulation in sub_data.manipulationToday.unique()[sub_data.manipulationToday.unique()!=None]:\n",
        "    # get the same RT data but only for post manipulations\n",
        "    relevantData = sub_data[(sub_data.manipulationToday == manipulation) & (sub_data.isUnderManipulation)]\n",
        "    summaryTable['Manipulation_MEANsecsToPress1_' + manipulation] = relevantData.secsToPress1.mean()\n",
        "    summaryTable['Manipulation_MEANsecsToPress2_' + manipulation] = relevantData.secsToPress2.mean()\n",
        "    summaryTable['Manipulation_MEANsecsToExitAfterOutcomeTime_' + manipulation] = relevantData[relevantData.secsToExitAfterOutcomeTime<=60].secsToExitAfterOutcomeTime.mean() # the latter is to exclude unreasonably higher values\n",
        "    summaryTable['Manipulation_MEANsecsToPress2fromEntry_' + manipulation] = relevantData.secsToPress2fromEntry.mean()\n",
        "    summaryTable['Manipulation_MEANsecsToExitAfterOutcomeTimefromEntry_' + manipulation] = relevantData[relevantData.secsToExitAfterOutcomeTime<=60].secsToExitAfterOutcomeTimefromEntry.mean() # the latter is to exclude unreasonably higher values\n",
        "    # get the same RT of the differences between before and after manipulations\n",
        "    relevantData2 = sub_data[(sub_data.manipulationToday == manipulation) & (sub_data.isUnderManipulation==False)]\n",
        "    summaryTable['After_minus_Before_Manipulation_MEANsecsToPress1_' + manipulation] = relevantData.secsToPress1.mean() - relevantData2.secsToPress1.mean()\n",
        "    summaryTable['After_minus_Before_Manipulation_MEANsecsToPress2_' + manipulation] = relevantData.secsToPress2.mean() - relevantData2.secsToPress2.mean()\n",
        "    summaryTable['After_minus_Before_Manipulation_MEANsecsToExitAfterOutcomeTime_' + manipulation] = relevantData[relevantData.secsToExitAfterOutcomeTime<=60].secsToExitAfterOutcomeTime.mean() - relevantData2[(relevantData2.activateManipulation != True) & (relevantData2.secsToExitAfterOutcomeTime<=60)].secsToExitAfterOutcomeTime.mean() # the latter is to exclude unreasonably higher values\n",
        "    summaryTable['After_minus_Before_Manipulation_MEANsecsToPress2fromEntry_' + manipulation] = relevantData.secsToPress2fromEntry.mean() - relevantData2.secsToPress2fromEntry.mean()\n",
        "    summaryTable['After_minus_Before_Manipulation_MEANsecsToExitAfterOutcomeTimefromEntry_' + manipulation] = relevantData[relevantData.secsToExitAfterOutcomeTime<=60].secsToExitAfterOutcomeTimefromEntry.mean() - relevantData2[(relevantData2.activateManipulation != True) & (relevantData2.secsToExitAfterOutcomeTime<=60)].secsToExitAfterOutcomeTimefromEntry.mean() # the latter is to exclude unreasonably higher values\n",
        "\n",
        "  # get RT data per day\n",
        "  for day in sub_data.day.unique():\n",
        "    day = int(day)\n",
        "    summaryTable['MEANsecsToPress1_day' + str(day)] = sub_data[sub_data.day==day].secsToPress1.mean()\n",
        "    summaryTable['MEANsecsToPress2_day' + str(day)] = sub_data[sub_data.day==day].secsToPress2.mean()\n",
        "    summaryTable['MEANsecsToExitAfterOutcomeTime_day' + str(day)] = sub_data[(sub_data.day==day) & (sub_data.activateManipulation != True) & (sub_data.secsToExitAfterOutcomeTime<=60)].secsToExitAfterOutcomeTime.mean() # the latter is to exclude unreasonably higher values\n",
        "    summaryTable['MEANsecsToPress2fromEntry_day' + str(day)] = sub_data[sub_data.day==day].secsToPress2fromEntry.mean()\n",
        "    summaryTable['MEANsecsToExitAfterOutcomeTimefromEntry_day' + str(day)] = sub_data[(sub_data.day==day) & (sub_data.activateManipulation != True) & (sub_data.secsToExitAfterOutcomeTime<=60)].secsToExitAfterOutcomeTimefromEntry.mean()\n",
        "\n",
        "\n",
        "  # Gather delta's data together:\n",
        "  summaryTable['subId'] = sub\n",
        "  if i == 0:\n",
        "    timeDeltaMeasures = summaryTable\n",
        "    nSessionByDay = sub_n_daily_sessions\n",
        "    nDailyEntries = sub_n_daily_entries\n",
        "  else:\n",
        "    timeDeltaMeasures = pd.concat([timeDeltaMeasures, summaryTable], axis=1)\n",
        "    nSessionByDay = nSessionByDay.append(sub_n_daily_sessions)\n",
        "    nDailyEntries = nDailyEntries.append(sub_n_daily_entries)\n",
        "\n",
        "# group together subjects data\n",
        "timeDeltaMeasures = timeDeltaMeasures.transpose().set_index('subId')\n",
        "timeDeltaMeasures.index = timeDeltaMeasures.index.astype('int') # making the index int instead of float\n",
        "\n",
        "# add 1s to relevant days with nan (which means that there was one session)\n",
        "nSessionByDay = nSessionByDay.reset_index('group')\n",
        "nDailyEntries = nDailyEntries.reset_index('group')\n",
        "\n",
        "# handle subjects with only one session on all days\n",
        "missing_subjects = list(sorted(set(sub_df_extended.subId.unique()) - set(nSessionByDay.index))) # this is for subjects who had only one session across all days.\n",
        "for missed_sub in missing_subjects:\n",
        "  rowToAdd = pd.DataFrame(np.nan, index=[missed_sub], columns=nSessionByDay.columns)\n",
        "  rowToAdd.group = sub_df_extended[sub_df_extended.subId == missed_sub].group.iloc[0]\n",
        "  rowToAdd.index.name = 'subId'\n",
        "  nSessionByDay = pd.concat([nSessionByDay, rowToAdd]).sort_index()\n",
        "\n",
        "for group in n_experimental_days.keys():\n",
        "  for c in range(1,n_experimental_days[group]+1):\n",
        "    nSessionByDay.loc[(nSessionByDay.group==group) & (nSessionByDay[c].isna()),c] = 1\n",
        "# Get the daily average entries per session\n",
        "avgEntriesPerSessionByDay = nDailyEntries.copy()\n",
        "avgEntriesPerSessionByDay.loc[:,[i for i in range(1,avgEntriesPerSessionByDay.columns[-1]+1)]] = avgEntriesPerSessionByDay.loc[:,[i for i in range(1,avgEntriesPerSessionByDay.columns[-1]+1)]] / nSessionByDay.loc[:,[i for i in range(1,nSessionByDay.columns[-1]+1)]]\n",
        "avgEntriesPerSessionByDay\n",
        "\n",
        "\n",
        "if input(\"Should avg. entries per session after manipulations turned to 0 instead of NA when there where no sessions? [y/N] \") == 'y':\n",
        "  print('Turn these values to zeros')\n",
        "  columnsWithManipulationsAvgSessionEntries = timeDeltaMeasures.columns[['avgSessionEntries_' in x for x in timeDeltaMeasures.columns]]\n",
        "  for col in columnsWithManipulationsAvgSessionEntries:\n",
        "    timeDeltaMeasures.loc[timeDeltaMeasures[col].isna(),col] = 0\n",
        "\n",
        "print('\\n********************************\\n created: timeDeltaMeasures + nSessionByDay and avgEntriesPerSessionByDay tables\\n********************************')\n",
        "print('Note: for the n_sessions, averageSessionsPerDay, and averageEntriesInSession (one score per participants) the entire day of devaluation is excluded.')\n",
        "print('Note: for the n_sessions_no_manipulations, averageSessionsPerDay_no_manipulations, and averageEntriesInSession_no_manipulations (one score per participants) the entire manipulations days are excluded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePG9Po22cz57"
      },
      "source": [
        "## Make a summary table of all data together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iVMoS_kgcuaY"
      },
      "outputs": [],
      "source": [
        "timeDeltaMeasures_temp = timeDeltaMeasures.copy()\n",
        "timeDeltaMeasures_temp.columns = ['timeDelta_'+ x for x in timeDeltaMeasures_temp.columns]\n",
        "timeDeltaMeasures_temp.index.name = 'subID'\n",
        "timeDeltaMeasures_temp.columns.name = 'manipulation'\n",
        "all_data_for_R = pd.concat([core_data_with_diffs.reset_index('group'), timeDeltaMeasures_temp], axis=1)\n",
        "all_data_for_R = all_data_for_R.reset_index().set_index(['subID','group'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HigqzNelLMxY"
      },
      "outputs": [],
      "source": [
        "#@title add n_session and avgSessionEntries diff measures { form-width: \"5%\" }\n",
        "\n",
        "def add_session_diff_measures(table, entryMeasure):\n",
        "  still_val_Measure = 'timeDelta_' + entryMeasure + '_still_valued'\n",
        "  deval_Measure = 'timeDelta_' + entryMeasure + '_devaluation'\n",
        "  still_val_post_deval_Measure = 'timeDelta_' + entryMeasure + '_still_valued_post_deval'\n",
        "\n",
        "  # add vars:\n",
        "  # add mean valued data:\n",
        "  table['mean_' + still_val_Measure] = table.loc[:,[still_val_Measure, still_val_post_deval_Measure]].mean(axis=1)\n",
        "  # add square root data (to give less weight for high values)\n",
        "  table[deval_Measure + '_SQRT'] = np.sqrt(table[deval_Measure])\n",
        "  table[still_val_Measure + '_SQRT'] = np.sqrt(table[still_val_Measure])\n",
        "  table[still_val_post_deval_Measure + '_SQRT'] = np.sqrt(table[still_val_post_deval_Measure])\n",
        "  table['mean_' + still_val_Measure + 'SQRT'] = np.sqrt(table['mean_' + still_val_Measure])\n",
        "\n",
        "  # add diff vars:\n",
        "  table[entryMeasure + '_postVal_minus_deval'] = table.loc[:,[deval_Measure, still_val_post_deval_Measure]].diff(axis=1).loc[:,still_val_post_deval_Measure]\n",
        "  table[entryMeasure + '_stillVal_minus_deval'] = table.loc[:,[deval_Measure, still_val_Measure]].diff(axis=1).loc[:,still_val_Measure]\n",
        "  table[entryMeasure + '_preVal_relativeDiff_deval'] = table.apply(lambda x: (x[still_val_Measure] - x[deval_Measure])/(x[deval_Measure] + x[still_val_Measure]) if (x[deval_Measure] + x[still_val_Measure]) != 0 else None, axis=1)\n",
        "  table[entryMeasure + '_postVal_relativeDiff_deval'] = table.apply(lambda x: (x[still_val_post_deval_Measure] - x[deval_Measure])/(x[deval_Measure] + x[still_val_post_deval_Measure]) if (x[deval_Measure] + x[still_val_post_deval_Measure]) != 0 else None, axis=1)\n",
        "  table[entryMeasure + '_preVal_relativeDiff_deval_SQRT'] = table.apply(lambda x: (x[still_val_Measure + '_SQRT'] - x[deval_Measure + '_SQRT'])/(x[deval_Measure + '_SQRT'] + x[still_val_Measure + '_SQRT']) if (x[deval_Measure + '_SQRT'] + x[still_val_Measure + '_SQRT']) != 0 else None, axis=1)\n",
        "  table[entryMeasure + '_postVal_relativeDiff_deval_SQRT'] = table.apply(lambda x: (x[still_val_post_deval_Measure + '_SQRT'] - x[deval_Measure + '_SQRT'])/(x[deval_Measure + '_SQRT'] + x[still_val_post_deval_Measure + '_SQRT']) if (x[deval_Measure + '_SQRT'] + x[still_val_post_deval_Measure + '_SQRT']) != 0 else None, axis=1)\n",
        "\n",
        "  table[entryMeasure + '_meanVal_minus_deval'] = table.loc[:,[deval_Measure, 'mean_' + still_val_Measure]].diff(axis=1).loc[:,'mean_' + still_val_Measure]\n",
        "  table[entryMeasure + '_meanVal_minus_deval_SQRT'] = table.loc[:,[deval_Measure + '_SQRT', 'mean_' + still_val_Measure + 'SQRT']].diff(axis=1).loc[:,'mean_' + still_val_Measure + 'SQRT']\n",
        "  table[entryMeasure + '_meanVal_relativeDiff_deval'] = table.apply(lambda x: (x['mean_' + still_val_Measure] - x[deval_Measure])/(x[deval_Measure] + x['mean_' + still_val_Measure]) if (x[deval_Measure] + x['mean_' + still_val_Measure]) != 0 else None, axis=1)\n",
        "  table[entryMeasure + '_meanVal_relativeDiff_deval_SQRT'] = table.apply(lambda x: (x['mean_' + still_val_Measure + 'SQRT'] - x[deval_Measure + '_SQRT'])/(x[deval_Measure + '_SQRT'] + x['mean_' + still_val_Measure + 'SQRT']) if (x[deval_Measure + '_SQRT'] + x['mean_' + still_val_Measure + 'SQRT']) != 0 else None, axis=1)\n",
        "\n",
        "  if input(\"Should RelativeDiff values for participants with 0 entries after all manipulatuions turned to 0 (rather than nan)? [y/N] \") == 'y':\n",
        "    print('Turn these values to zeros')\n",
        "    columnsWithRelativeDiff = table.columns[['relativeDiff' in x for x in table.columns]]\n",
        "    for col in columnsWithRelativeDiff:\n",
        "      table.loc[table[col].isna(),col] = 0\n",
        "\n",
        "  table.tail()\n",
        "\n",
        "  return table\n",
        "\n",
        "all_data_for_R = add_session_diff_measures(table=all_data_for_R, entryMeasure = 'n_sessions')\n",
        "\n",
        "all_data_for_R = add_session_diff_measures(table=all_data_for_R, entryMeasure = 'avgSessionEntries')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cellView": "form",
        "id": "u6r_2k4hOsvX"
      },
      "outputs": [],
      "source": [
        "#@title { form-width: \"5%\" }\n",
        "#@markdown Assemble and add entries variability measures for n_sessions and avgSessionEntries\n",
        "\n",
        "# get number of entries in each day\n",
        "n_sessions_by_day = nSessionByDay.drop(['group'],axis=1)\n",
        "# remove data for days that shoudn't be in the calculation\n",
        "n_sessions_by_day.loc[100:199, main_manipulation_days['short_training']['devaluation']] = None\n",
        "n_sessions_by_day.loc[200:400, main_manipulation_days['long_training']['devaluation']] = None\n",
        "n_sessions_by_day\n",
        "\n",
        "# add the relevant data\n",
        "all_data_for_R = all_data_for_R.assign(n_sessions_STD_entriesAcrossNoDevalDays=n_sessions_by_day.std(axis=1,skipna=True).values)\n",
        "all_data_for_R = all_data_for_R.assign(n_sessions_SEM_entriesAcrossNoDevalDays=n_sessions_by_day.sem(axis=1,skipna=True).values)\n",
        "\n",
        "all_data_for_R = all_data_for_R.assign(n_sessions_NormedSTD_entriesAcrossNoDevalDays=n_sessions_by_day.std(axis=1,skipna=True).values / n_sessions_by_day.mean(axis=1,skipna=True).values)\n",
        "all_data_for_R = all_data_for_R.assign(n_sessions_NormedVar_entriesAcrossNoDevalDays=n_sessions_by_day.var(axis=1,skipna=True).values / n_sessions_by_day.mean(axis=1,skipna=True).values)\n",
        "\n",
        "# Add first day entries and entries throught the experiment\n",
        "all_data_for_R = all_data_for_R.assign(firstDaySessions=n_sessions_by_day[1].values)\n",
        "all_data_for_R = all_data_for_R.assign(allSessions=n_sessions_by_day.sum(axis=1,skipna=True).values)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# get number of entries in each day\n",
        "avgSessionEntries_by_day = avgEntriesPerSessionByDay.drop(['group'],axis=1)\n",
        "# remove data for days that shoudn't be in the calculation\n",
        "avgSessionEntries_by_day.loc[100:199,main_manipulation_days['short_training']['devaluation']] = None\n",
        "avgSessionEntries_by_day.loc[200:400,main_manipulation_days['long_training']['devaluation']] = None\n",
        "avgSessionEntries_by_day\n",
        "\n",
        "# add the relevant data\n",
        "all_data_for_R = all_data_for_R.assign(avgSessionEntries_STD_entriesAcrossNoDevalDays=avgSessionEntries_by_day.std(axis=1,skipna=True).values)\n",
        "all_data_for_R = all_data_for_R.assign(avgSessionEntries_SEM_entriesAcrossNoDevalDays=avgSessionEntries_by_day.sem(axis=1,skipna=True).values)\n",
        "\n",
        "all_data_for_R = all_data_for_R.assign(avgSessionEntries_NormedSTD_entriesAcrossNoDevalDays=avgSessionEntries_by_day.std(axis=1,skipna=True).values / avgSessionEntries_by_day.mean(axis=1,skipna=True).values)\n",
        "all_data_for_R = all_data_for_R.assign(avgSessionEntries_NormedVar_entriesAcrossNoDevalDays=avgSessionEntries_by_day.var(axis=1,skipna=True).values / avgSessionEntries_by_day.mean(axis=1,skipna=True).values)\n",
        "\n",
        "# Add first day entries and entries throught the experiment\n",
        "all_data_for_R = all_data_for_R.assign(firstDayAvgSessionEntries=avgSessionEntries_by_day[1].values)\n",
        "all_data_for_R['allAvgSessionEntries'] = all_data_for_R['timeDelta_avgSessionEntries']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiNE8vmci8bO"
      },
      "source": [
        "## Put data together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-ekfKrc0i_AI"
      },
      "outputs": [],
      "source": [
        "# add what's added to the core_data_with_diffs and then save\n",
        "all_data_for_R = pd.concat([all_data_for_R, core_data_with_diffs.loc[:,~core_data_with_diffs.columns.isin(all_data_for_R.columns)]], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYz9n5rIXPUU"
      },
      "source": [
        "# Consumption Test (note a manual fix in the code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aWsD0cJ4bdSK"
      },
      "outputs": [],
      "source": [
        "#@title parse Consumption Test Data (note one manual correction based on the raw data) { form-width: \"5%\" }\n",
        "consumptionTestData = data.sub_df[(data.sub_df.consumptionTest==True) & (~data.sub_df['foundCaveConfirmationTime'].isnull())]\n",
        "consumptionTestData = consumptionTestData.drop(['touchData', 'screenOrientationData'], axis=1)\n",
        "\n",
        "# A specific MANUAL fix for when there was no coin_task_finish_status (but there is data) for one subject\n",
        "problems = consumptionTestData[consumptionTestData.coin_task_finish_status.isna()]\n",
        "if problems.subId.unique() == 233 and problems.day.unique() == 9:\n",
        "  consumptionTestData.loc[consumptionTestData.coin_task_finish_status.isna(),'coin_task_finish_status'] = [{'finish_time': None, 'total_gold_collected': 13, 'total_presses': 13}]\n",
        "\n",
        "consumptionTestData.loc[consumptionTestData.coins_task_misses_count.isnull(), 'coins_task_misses_count'] = 0  # turn back NaNs in of task misses to 0;\n",
        "# the next line is a fix needed for when loading the data from a file (and not from the server)\n",
        "consumptionTestData.coin_task_finish_status = consumptionTestData.coin_task_finish_status.map(lambda x: x if isinstance(x,dict) else json.loads(x.replace(\"'\", '\"')))\n",
        "consumptionTestData['gold'] = consumptionTestData.coin_task_finish_status.map(lambda x: x['total_gold_collected'])\n",
        "consumptionTestData['rocks'] = consumptionTestData.coin_task_finish_status.map(lambda x: x['total_presses'] - x['total_gold_collected']) - consumptionTestData['coins_task_misses_count']\n",
        "consumptionTestData['misses'] = consumptionTestData['coins_task_misses_count']\n",
        "consumptionTestData['completionTime'] = consumptionTestData.coin_task_finish_status.map(lambda x: x['finish_time'])\n",
        "\n",
        "consumptionTestData = consumptionTestData[['subId','group','day','manipulationToday','gold','rocks','misses', 'completionTime']]\n",
        "consumptionTestData = consumptionTestData.rename(columns={'manipulationToday': 'manipulation'})\n",
        "consumptionTestData = consumptionTestData.reset_index()\n",
        "consumptionTestData = consumptionTestData.drop(columns='index')\n",
        "\n",
        "# Make sure in the other stuff they were enterd\n",
        "print('n subjects:', len(consumptionTestData.subId.unique()))\n",
        "print('n subjects with consumption test on devaluation:', len(consumptionTestData[consumptionTestData.manipulation=='devaluation'].subId.unique()))\n",
        "print('n subjects with consumption test on still_valued:', len(consumptionTestData[consumptionTestData.manipulation=='still_valued'].subId.unique()))\n",
        "print('n subjects with consumption test on still_valued_post_deval:', len(consumptionTestData[consumptionTestData.manipulation=='still_valued_post_deval'].subId.unique()))\n",
        "\n",
        "# create a list of subjects that did not commit at all the cave on the devaluation days:\n",
        "subsNotEntered = []\n",
        "for sub in consumptionTestData.subId.unique():\n",
        "  subGroup = consumptionTestData[consumptionTestData.subId == sub].group.iloc[0]\n",
        "  for manip in manipulations_renamed[subGroup]:\n",
        "    if consumptionTestData[(consumptionTestData.subId == sub) & (consumptionTestData.manipulation == manip)].empty:\n",
        "      print(f'Adding 0 for: {sub} - {subGroup} - {manip}')\n",
        "      newRow = {'subId': sub,\t'group': subGroup,\t'day': all_manipulation_days[subGroup][manip], 'manipulation':manip, 'gold':0,'rocks':0,'misses':\t0,'completionTime': None}\n",
        "      consumptionTestData = consumptionTestData.append(newRow, ignore_index=True)\n",
        "      if manip == 'devaluation':\n",
        "        subsNotEntered.append(sub)\n",
        "\n",
        "consumptionTestData = consumptionTestData.sort_values(by=['subId', 'day'])\n",
        "consumptionTestData = consumptionTestData.reset_index(drop=True)\n",
        "\n",
        "print('\\n\\nsubjects that did not committed the task on devaluation:', subsNotEntered)\n",
        "print('pointed at the data as an entry with 0 gold.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellView": "form",
        "id": "WQA_kW21uzvH"
      },
      "outputs": [],
      "source": [
        "#@title Check eligibility of consumption tests\n",
        "for subID in data.sub_df.subId.unique():\n",
        "  if subID not in consumptionTestData.subId.unique():\n",
        "    print('>>> subject ' + str(subID) + ' has 0 completed consumption tests.')\n",
        "for subID in consumptionTestData.subId.unique():\n",
        "  if len(consumptionTestData[consumptionTestData.subId == subID]) == 1 :\n",
        "    print('>>> subject ' + str(subID) + ' has only 1 completed consumption tests in ' + consumptionTestData[consumptionTestData.subId == subID].manipulation.iloc[0].upper() + '.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv-Neip5mZkL"
      },
      "source": [
        "### Add the consumption test data to the main data for variable and file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fCEGV_-YhBQl"
      },
      "outputs": [],
      "source": [
        "all_data_for_R2 = all_data_for_R.reset_index()\n",
        "\n",
        "for index, row in consumptionTestData.iterrows():\n",
        "  all_data_for_R2.loc[all_data_for_R2.subID == row['subId'], f\"cave_gold_{row['manipulation']}\"] = row['gold']\n",
        "\n",
        "all_data_for_R = all_data_for_R2.set_index(['subID','group'])\n",
        "\n",
        "all_data_for_R.to_csv(allDataFor_R_File)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecslkjvZSSq8"
      },
      "source": [
        "# Save raw data - use only when need it because it initialize the data variable\n",
        "* If run on Colab, the files formed here are formed on line and then should be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellView": "form",
        "id": "x9ClZO3CSKLe"
      },
      "outputs": [],
      "source": [
        "#@markdown Save Raw Data { form-width: \"5%\" }\n",
        "\n",
        "import pickle\n",
        "# # Save raw data as a list\n",
        "data_temp = Data(minSubID, maxSubID) # instanciates and creates the data.raw\n",
        "with open(rawDataAsListFile, 'w') as f:\n",
        "    for item in data_temp.raw:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "data_temp.filter_trials_data()  # creates data.trials_data\n",
        "data_temp.create_sub_dict()  # creates data.sub_dict (defaulted to create it from data.trials_data)\n",
        "# save raw data binary\n",
        "a_file = open(rawDataAsBinaryFile, \"wb\")\n",
        "pickle.dump(data_temp.sub_dict, a_file)\n",
        "a_file.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
